{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *In silico* evolution of promoters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable GPU memory growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to one-hot encode the DNA sequences (adapted from https://colab.research.google.com/drive/17E4h5aAOioh5DiTo7MZg4hpL6Z_0FyWr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoder = LabelEncoder()  \n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "\n",
    "def one_hot_encoding(sequences, verbose = True): \n",
    "    one_hot_sequences = []\n",
    "\n",
    "    if verbose:\n",
    "        i = 0\n",
    "        print('one-hot encoding in progress ...', flush = True)\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        integer_encoded = integer_encoder.fit_transform(list(sequence))\n",
    "        integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "        one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "        one_hot_sequences.append(one_hot_encoded.toarray())\n",
    "    \n",
    "        if verbose:\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i, 'sequences processed', flush = True, end = '\\r')\n",
    "        \n",
    "    if verbose:\n",
    "        print('finished one-hot encoding:', i, 'sequences processed', flush = True)\n",
    "    \n",
    "    one_hot_sequences = np.stack(one_hot_sequences)\n",
    "\n",
    "    return one_hot_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to reverse the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(one_hot_sequences, verbose = True):\n",
    "    sequences = []\n",
    "    \n",
    "    if verbose:\n",
    "        i = 0\n",
    "        print('reversing one-hot encoding ...', flush = True)\n",
    "    \n",
    "    for one_hot_sequence in one_hot_sequences:\n",
    "        integer_encoded = one_hot_encoder.inverse_transform(one_hot_sequence)\n",
    "        integer_encoded = integer_encoded.flatten()\n",
    "        sequence = integer_encoder.inverse_transform(integer_encoded)\n",
    "        sequence = ''.join(sequence)\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "        if verbose:\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i, 'sequences processed', flush = True, end = '\\r')\n",
    "                \n",
    "    if verbose:\n",
    "        print('finshed reverse one-hot encoding:', i, 'sequences processed', flush = True)\n",
    "    \n",
    "    return(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate all possible point mutations in a one-hot encoded sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bases = one_hot_encoding(['ACGT'], verbose = False)\n",
    "base_dict = dict(zip(['A', 'C', 'G', 'T'], bases[0]))\n",
    "\n",
    "def mutate_seq(sequence):\n",
    "    mutated_seqs = [sequence]\n",
    "    for pos in range(len(sequence)):\n",
    "        for base in base_dict.values():\n",
    "            if not(np.array_equal(sequence[pos], base)):\n",
    "                mut_seq = np.concatenate((sequence[:pos], base.reshape(1, -1), sequence[pos+1:]))\n",
    "                mutated_seqs.append(mut_seq)\n",
    "                \n",
    "    mutated_seqs = np.stack(mutated_seqs)\n",
    "    \n",
    "    return(mutated_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to score sequences with models for both expression systems and pick the best sequence for the indicated system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seqs(sequences, system):\n",
    "    prediction_leaf = model_leaf.predict(sequences)\n",
    "    prediction_proto = model_proto.predict(sequences)\n",
    "    if system == 'leaf':\n",
    "        best_id = prediction_leaf.argmax()\n",
    "    if system == 'proto':\n",
    "        best_id = prediction_proto.argmax()\n",
    "    if system == 'both':\n",
    "        prediction_mean = (prediction_leaf + prediction_proto) / 2\n",
    "        best_id = prediction_mean.argmax()\n",
    "    \n",
    "    return(sequences[best_id], prediction_leaf[best_id][0], prediction_proto[best_id][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to remove restriction enzyme sites from a sequence by mutating the site and picking the best performing sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define restriction enzyme sites as regular expression.\n",
    "# also considers sites that would be formed with the cloning adapter (5' G and 3' CC)\n",
    "REsites = re.compile(r'((G|^)GTCT(C|$))|((G|^)AGA(CC|C$|$))|((G|^)AAGA(C|$))|((G|^)TCTT(C|$))')\n",
    "\n",
    "# helper function to mutate sequences between 'start' and 'stop' \n",
    "def mutate_region(sequences, start, stop):\n",
    "    mut_seqs = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        for pos in range(start, stop):\n",
    "            for base in ['A', 'C', 'G', 'T']:\n",
    "                if sequence[pos] != base:\n",
    "                    mut_seq = sequence[:pos] + base + sequence[pos+1:]\n",
    "                    mut_seqs.append(mut_seq)\n",
    "                \n",
    "    return (mut_seqs)\n",
    "\n",
    "# function to find all matches to restriction enzyme sites, mutate them, and select the best performing sequence\n",
    "def fix_REsite(sequence, system):\n",
    "    matches = REsites.finditer(sequence)\n",
    "    mut_seqs = [sequence]\n",
    "    \n",
    "    # introduce mutations to destroy RE sites\n",
    "    for match in matches:\n",
    "        mut_seqs = mutate_region(mut_seqs, match.start(), match.end())\n",
    "        \n",
    "    # filter out sequences where a new RE site was introduced\n",
    "    mut_seqs = [seq for seq in mut_seqs if not REsites.search(seq)]\n",
    "    \n",
    "    # one-hot encode mutated sequences\n",
    "    one_hot_seqs = one_hot_encoding(mut_seqs, verbose = False)\n",
    "    \n",
    "    # score sequences and pick best\n",
    "    fixed_seq, leaf, proto = score_seqs(one_hot_seqs, system)\n",
    "    \n",
    "    # reverse one-hot encoding of fixed sequence\n",
    "    fixed_seq = reverse_one_hot([fixed_seq], verbose = False)[0]\n",
    "        \n",
    "    return(fixed_seq, leaf, proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_leaf = k.models.load_model('model_leaf')\n",
    "model_proto = k.models.load_model('model_proto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_promoters = pd.read_csv('../analysis/validation_sequences/promoters_for_evolution.tsv', sep = '\\t', header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode start sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding in progress ...\n",
      "finished one-hot encoding: 310 sequences processed\n"
     ]
    }
   ],
   "source": [
    "start_seqs = one_hot_encoding(original_promoters['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict promoter strength of start sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D1BCF4BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D1BCF4BE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D1B7970A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D1B7970A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "prediction_leaf = model_leaf.predict(start_seqs)\n",
    "prediction_proto = model_proto.predict(start_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up dataframe for results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_data = pd.DataFrame(data = {\n",
    "    'round' : 0,\n",
    "    'origin' : original_promoters['name'],\n",
    "    'opt_for' : 'start',\n",
    "    'sequence' : list(start_seqs),\n",
    "    'pred_leaf' : prediction_leaf.flatten(),\n",
    "    'pred_proto' : prediction_proto.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform iterative evolution of the promoter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing promoters for: leaf\n",
      "performed 10 rounds of evolution\n",
      "optimizing promoters for: proto\n",
      "performed 10 rounds of evolution\n",
      "optimizing promoters for: both\n",
      "performed 10 rounds of evolution\n"
     ]
    }
   ],
   "source": [
    "n_rounds = 10\n",
    "\n",
    "for system in ['leaf', 'proto', 'both']:\n",
    "    print('optimizing promoters for:', system, flush = True)\n",
    "\n",
    "    for thisround in range(1, n_rounds + 1):        \n",
    "        lastround = thisround - 1\n",
    "        data = evolution_data[evolution_data['round'] == lastround]\n",
    "\n",
    "        if thisround == 1:\n",
    "            data = data[data['opt_for'] == 'start']\n",
    "        else:\n",
    "            data = data[data['opt_for'] == system]\n",
    "            \n",
    "        if data.empty:\n",
    "            break\n",
    "            \n",
    "        origins = data['origin']\n",
    "        sequences = data['sequence']\n",
    "            \n",
    "        for origin, sequence in zip(origins, sequences):\n",
    "            mut_seqs = mutate_seq(sequence)\n",
    "            opt_seq, leaf, proto = score_seqs(mut_seqs, system)\n",
    "            \n",
    "            if not(np.array_equal(opt_seq, sequence)):\n",
    "                evolution_data.loc[len(evolution_data)] = [thisround, origin, system, opt_seq, leaf, proto]\n",
    "        \n",
    "        print('finished round', thisround, flush = True, end = '\\r')\n",
    "        \n",
    "    print('performed', thisround, 'rounds of evolution', flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversing one-hot encoding ...\n",
      "finshed reverse one-hot encoding: 9610 sequences processed\n"
     ]
    }
   ],
   "source": [
    "evolution_data['sequence'] = reverse_one_hot(evolution_data['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate if the sequence contains a restriction enzyme site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_data['REsite'] = [bool(REsites.search(x)) for x in evolution_data['sequence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select sequences that need to be fixed (third or final round and contains a restriction enzyme site):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seqs = evolution_data[((evolution_data['round'] == 3) | (evolution_data['round'] == max(evolution_data['round']))) & (evolution_data['REsite'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seqs[['sequence', 'pred_leaf', 'pred_proto']] = [fix_REsite(seq, sys) for (seq, sys) in zip(fix_seqs['sequence'], fix_seqs['opt_for'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine fixed sequences with the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_data = fix_seqs.combine_first(evolution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'round' column to integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_data = evolution_data.astype({'round' : int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_data.to_csv('evolution_data.tsv', sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CNN2.2]",
   "language": "python",
   "name": "conda-env-CNN2.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
